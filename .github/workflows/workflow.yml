name: dticket-scraper
on:
  schedule:
    - cron: '0/40 * * * *'
env:
  SELENIUM_URL: ${{secrets.SELENIUM_URL}}
  DATABASE_URL: ${{secrets.DATABASE_URL}}
  SCRAPING_TARGET_URL: ${{secrets.SCRAPING_TARGET_URL}}
  LINE_ACCESS_TOKEN: ${{secrets.LINE_ACCESS_TOKEN}}
  LINE_ADMIN_ID: ${{secrets.LINE_ADMIN_ID}}
  TWITTER_API_KEY: ${{secrets.TWITTER_API_KEY}}
  TWITTER_API_SECRET: ${{secrets.TWITTER_API_SECRET}}
  TWITTER_ACCESS_TOKEN: ${{secrets.TWITTER_ACCESS_TOKEN}}
  TWITTER_ACCESS_TOKEN_SECRET: ${{secrets.TWITTER_ACCESS_TOKEN_SECRET}}
  TWITTER_API_KEY_WEEKDAY: ${{secrets.TWITTER_API_KEY_WEEKDAY}}
  TWITTER_API_SECRET_WEEKDAY: ${{secrets.TWITTER_API_SECRET_WEEKDAY}}
  TWITTER_ACCESS_TOKEN_WEEKDAY: ${{secrets.TWITTER_ACCESS_TOKEN_WEEKDAY}}
  TWITTER_ACCESS_TOKEN_SECRET_WEEKDAY: ${{secrets.TWITTER_ACCESS_TOKEN_SECRET_WEEKDAY}}
jobs:
  build:
    name: scraping
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - run: docker-compose up -d
      - run: sleep 5
      - run: docker-compose exec -T app python main.py
      - run: docker-compose down
